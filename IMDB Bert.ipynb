{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bert.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNeqgl9uqwuN6pWzhBixo0n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SooWanKim/practice_nlp/blob/master/IMDB%20Bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbkG43nxNyHU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "3168a015-7aa2-4b40-972b-25481058b9a4"
      },
      "source": [
        "!pip install tokenizers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tokenizers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/ee/fedc3509145ad60fe5b418783f4a4c1b5462a4f0e8c7bbdbda52bdcda486/tokenizers-0.8.1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 8.5MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.8.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mY1thwYUN2ya",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "outputId": "83d620ee-f0d1-4644-b486-feea87c3ba51"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\r\u001b[K     |▍                               | 10kB 29.0MB/s eta 0:00:01\r\u001b[K     |▉                               | 20kB 6.0MB/s eta 0:00:01\r\u001b[K     |█▎                              | 30kB 7.1MB/s eta 0:00:01\r\u001b[K     |█▊                              | 40kB 7.7MB/s eta 0:00:01\r\u001b[K     |██▏                             | 51kB 7.1MB/s eta 0:00:01\r\u001b[K     |██▋                             | 61kB 8.0MB/s eta 0:00:01\r\u001b[K     |███                             | 71kB 7.8MB/s eta 0:00:01\r\u001b[K     |███▍                            | 81kB 8.8MB/s eta 0:00:01\r\u001b[K     |███▉                            | 92kB 8.0MB/s eta 0:00:01\r\u001b[K     |████▎                           | 102kB 8.2MB/s eta 0:00:01\r\u001b[K     |████▊                           | 112kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 122kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 133kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████                          | 143kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 153kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 163kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 174kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 184kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████                        | 194kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 204kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████                       | 215kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 225kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 235kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 245kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 256kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████                     | 266kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 276kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████                    | 286kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 296kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 307kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 317kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 327kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 337kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 348kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 358kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 368kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 378kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 389kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 399kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 409kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 419kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 430kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 440kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 450kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 460kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 471kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 481kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 491kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 501kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 512kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 522kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 532kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 542kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 552kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 563kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 573kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 583kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 593kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 604kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 614kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 624kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 634kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 645kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 655kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 665kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 675kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 686kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 696kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 706kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 716kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 727kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 737kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 747kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 757kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 768kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 778kB 8.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 24.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 51.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 59.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=b0e383380cfd41b2e3d653dc606f0904a16f04fcfa4323f323ba7445022d45cf\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "  Found existing installation: tokenizers 0.8.1\n",
            "    Uninstalling tokenizers-0.8.1:\n",
            "      Successfully uninstalled tokenizers-0.8.1\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXA9vSzcjBv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import os\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
        "from google.colab import drive\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc0M6LcQj_Ee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SEQ_LEN = 100  # max sequence length\n",
        "\n",
        "\n",
        "def get_masks(tokens):\n",
        "    \"\"\"Masks: 1 for real tokens and 0 for paddings\"\"\"\n",
        "    return [1] * len(tokens) + [0] * (MAX_SEQ_LEN - len(tokens))\n",
        "\n",
        "\n",
        "def get_segments(tokens):\n",
        "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
        "    segments = []\n",
        "    current_segment_id = 0\n",
        "    for token in tokens:\n",
        "        segments.append(current_segment_id)\n",
        "        if token == \"[SEP]\":\n",
        "            current_segment_id = 1\n",
        "    return segments + [0] * (MAX_SEQ_LEN - len(tokens))\n",
        "\n",
        "\n",
        "def get_ids(tokens, ids):\n",
        "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
        "    token_ids = ids\n",
        "    input_ids = token_ids + [0] * (MAX_SEQ_LEN - len(token_ids))\n",
        "    return input_ids\n",
        "\n",
        "\n",
        "def create_single_input(sentence, tokenizer, max_len):\n",
        "    \"\"\"Create an input from a sentence\"\"\"\n",
        "\n",
        "    encoded = tokenizer.encode(sentence)\n",
        "\n",
        "    ids = get_ids(encoded.tokens, encoded.ids)\n",
        "    masks = get_masks(encoded.tokens)\n",
        "    segments = get_segments(encoded.tokens)\n",
        "\n",
        "    return ids, masks, segments\n",
        "\n",
        "\n",
        "def convert_sentences_to_features(sentences, tokenizer):\n",
        "    \"\"\"Convert sentences to features: input_ids, input_masks and input_segments\"\"\"\n",
        "    input_ids, input_masks, input_segments = [], [], []\n",
        "\n",
        "    for sentence in tqdm(sentences, position=0, leave=True):\n",
        "        ids, masks, segments = create_single_input(sentence, tokenizer, MAX_SEQ_LEN - 2)\n",
        "        assert len(ids) == MAX_SEQ_LEN\n",
        "        assert len(masks) == MAX_SEQ_LEN\n",
        "        assert len(segments) == MAX_SEQ_LEN\n",
        "        input_ids.append(ids)\n",
        "        input_masks.append(masks)\n",
        "        input_segments.append(segments)\n",
        "\n",
        "    return [np.asarray(input_ids, dtype=np.int32), np.asarray(input_masks, dtype=np.int32), np.asarray(input_segments, dtype=np.int32)]\n",
        "\n",
        "\n",
        "def nlp_model(callable_object):\n",
        "    # Load the pre-trained BERT base model\n",
        "    bert_layer = hub.KerasLayer(handle=callable_object, trainable=True)\n",
        "\n",
        "    # BERT layer three inputs: ids, masks and segments\n",
        "    input_ids = Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name=\"input_ids\")\n",
        "    input_masks = Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name=\"input_masks\")\n",
        "    input_segments = Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name=\"segment_ids\")\n",
        "\n",
        "    inputs = [input_ids, input_masks, input_segments]  # BERT inputs\n",
        "    pooled_output, sequence_output = bert_layer(inputs)  # BERT outputs\n",
        "\n",
        "    # Add a hidden layer\n",
        "    x = Dense(units=768, activation=\"relu\")(pooled_output)\n",
        "    x = Dropout(0.1)(x)\n",
        "\n",
        "    # Add output layer\n",
        "    outputs = Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "    # Construct a new model\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77FmWqyuOK4V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        },
        "outputId": "10e2a4a8-9bf6-406c-af22-9512d7c2b0ae"
      },
      "source": [
        "drive.mount('/content/drive')\n",
        "model = nlp_model(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\")\n",
        "model.summary()\n",
        "\n",
        "# https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
        "# google drive 에 올려서 씀 ㅋ\n",
        "!ls '/content/drive/My Drive/Colab Notebooks/'\n",
        "\n",
        "movie_reviews = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/IMDB Dataset.csv\")\n",
        "movie_reviews.head(5)\n",
        "movie_reviews = movie_reviews.sample(frac=1) \n",
        "\n",
        "print(movie_reviews.isnull().values.any())\n",
        "print(movie_reviews.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Using /tmp/tfhub_modules to cache modules.\n",
            "INFO:absl:Downloading TF-Hub Module 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Downloaded https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2, Total size: 421.50MB\n",
            "INFO:absl:Downloaded TF-Hub Module 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_ids (InputLayer)          [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_masks (InputLayer)        [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)        [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "keras_layer (KerasLayer)        [(None, 768), (None, 109482241   input_ids[0][0]                  \n",
            "                                                                 input_masks[0][0]                \n",
            "                                                                 segment_ids[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 768)          590592      keras_layer[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 768)          0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            1538        dropout[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 110,074,371\n",
            "Trainable params: 110,074,370\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n",
            "'beginner.ipynb의 사본'  'Bert.ipynb의 사본'   nlp_model.h5\n",
            " Bert.ipynb\t\t 'IMDB Dataset.csv'\n",
            "False\n",
            "(50000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCk2nwjcONfT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "5e1a8e0e-4691-48c9-d5c5-99ee6a12cf9a"
      },
      "source": [
        "def preprocess_text(sen):\n",
        "    # Removing html tags\n",
        "    sentence = remove_tags(sen)\n",
        "\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence)\n",
        "\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", \" \", sentence)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r\"\\s+\", \" \", sentence)\n",
        "\n",
        "    return sentence\n",
        "\n",
        "\n",
        "TAG_RE = re.compile(r\"<[^>]+>\")\n",
        "\n",
        "\n",
        "def remove_tags(text):\n",
        "    return TAG_RE.sub(\"\", text)\n",
        "\n",
        "\n",
        "reviews = []\n",
        "sentences = list(movie_reviews[\"review\"])\n",
        "for sen in sentences:\n",
        "    reviews.append(preprocess_text(sen))\n",
        "\n",
        "print(movie_reviews.columns.values)\n",
        "print(movie_reviews.sentiment.unique())\n",
        "\n",
        "y = movie_reviews[\"sentiment\"]\n",
        "\n",
        "y = np.array(list(map(lambda x: 1 if x == \"positive\" else 0, y)))\n",
        "print(y[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['review' 'sentiment']\n",
            "['negative' 'positive']\n",
            "[0 0 0 1 0 1 1 0 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wovIvvQ3Pt_Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "e8d3fc0d-5e66-4753-a5d8-96e2cee183e0"
      },
      "source": [
        "slow_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "save_path = \"bert_base_uncased/\"\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "slow_tokenizer.save_pretrained(save_path)\n",
        "\n",
        "# Load the fast tokenizer from saved file\n",
        "tokenizer = BertWordPieceTokenizer(\"bert_base_uncased/vocab.txt\", lowercase=True)\n",
        "tokenizer.enable_truncation(MAX_SEQ_LEN - 2)\n",
        "\n",
        "train_count = 20000 \n",
        "test_count = 2000 \n",
        "\n",
        "# X_train = convert_sentences_to_features(reviews[:40000], tokenizer)\n",
        "# X_test = convert_sentences_to_features(reviews[40000:], tokenizer)\n",
        "\n",
        "X_train = convert_sentences_to_features(reviews[:train_count], tokenizer)\n",
        "X_test = convert_sentences_to_features(reviews[train_count:train_count+test_count], tokenizer)\n",
        "\n",
        "one_hot_encoded = to_categorical(y)\n",
        "# one_hot_encoded = tf.one_hot(y, 1)\n",
        "\n",
        "# y_train = one_hot_encoded[:40000]\n",
        "# y_test = one_hot_encoded[40000:]\n",
        "\n",
        "Y_train = one_hot_encoded[:train_count]\n",
        "Y_test = one_hot_encoded[train_count:train_count + test_count]\n",
        "print(one_hot_encoded[:10])\n",
        "print(len(one_hot_encoded))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 20000/20000 [00:10<00:00, 1976.01it/s]\n",
            "100%|██████████| 2000/2000 [00:01<00:00, 1997.76it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]]\n",
            "50000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhKp-AtQP4MZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "87e2ebfb-aa90-47e6-b98d-319072ff9f3f"
      },
      "source": [
        "BATCH_SIZE = 8\n",
        "EPOCHS = 1\n",
        "\n",
        "opt = Adam(learning_rate=2e-5)\n",
        "model.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1)\n",
        "\n",
        "model.save('nlp_model.h5')\n",
        "\n",
        "pred_test = np.argmax(model.predict(X_test), axis=1)\n",
        "print(pred_test[:10])\n",
        "# print(reviews[40000:40010])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2500/2500 [==============================] - 313s 125ms/step - loss: 0.2220 - accuracy: 0.9157 - val_loss: 0.2872 - val_accuracy: 0.8840\n",
            "[1 1 0 0 1 1 0 1 1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrO7XZ_RDJTt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "new_model = load_model('nlp_model.h5',custom_objects={'KerasLayer':hub.KerasLayer})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piB14Xm5EXgv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "outputId": "03eec6df-b4e9-4fe3-e423-063e6f28fc22"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(reviews[train_count:train_count+1])\n",
        "pred_test = np.argmax(new_model.predict(X_test), axis=1)\n",
        "print(X_test[:1])\n",
        "print(pred_test[:10])\n",
        "print(Y_test[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['This movie even though is about one of the most favorite topics of Mexican producers producers the extreme life in our cities has funny way to put it on the screen Four of the more important Mexican directors of the last times approach histories of our city framed in diverse literary sorts as it can be the farce or the satire which gives us film with over exposed topic in our country but narrated in very different way which gives freshness tone him With actors little known but that interprets of excellent way their paper each one of the directors reflect in the stories the capacity by we have been identified anywhere in the world that capacity of laugh the pains and to make celebration of the sadness Perhaps to many people in our country the film not have pleased but consider that people of other countries could find attractive and share the surrealism of the Mexican ']\n",
            "[array([[  101,  2023,  3185, ...,   102,     0,     0],\n",
            "       [  101,  2023,  2003, ...,     0,     0,     0],\n",
            "       [  101,  5199,  4202, ...,   102,     0,     0],\n",
            "       ...,\n",
            "       [  101,  2023,  2003, ...,   102,     0,     0],\n",
            "       [  101,  1045,  2310, ...,   102,     0,     0],\n",
            "       [  101, 13749, 10626, ...,   102,     0,     0]], dtype=int32)]\n",
            "[1 1 0 0 1 1 0 1 1 0]\n",
            "[[0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpHnJdz3EcJT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "46cec85f-279c-445e-849d-d9a4efada62e"
      },
      "source": [
        "print(classification_report(np.argmax(Y_test,axis=1), pred_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.86      0.88      1000\n",
            "           1       0.87      0.91      0.89      1000\n",
            "\n",
            "    accuracy                           0.88      2000\n",
            "   macro avg       0.88      0.88      0.88      2000\n",
            "weighted avg       0.88      0.88      0.88      2000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tokr6G5InncH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "4ca3d0d5-13bb-4c7e-bbcd-564970e75ec4"
      },
      "source": [
        "def sentiment_predict(sentence, tokenizer):\n",
        "    x_test = convert_sentences_to_features(sentence, tokenizer)\n",
        "    pred = np.argmax(model.predict(x_test), axis=1)\n",
        "    print('\\n')\n",
        "    print(pred)\n",
        "\n",
        "    for score in pred:\n",
        "        score = float(score)  # 예측\n",
        "        if score > 0.0:\n",
        "            print(\"긍정 리뷰입니다.\\n\")\n",
        "        else:\n",
        "            print(\"부정 리뷰입니다.\\n\")\n",
        "\n",
        "\n",
        "sentiment_predict(reviews[train_count:train_count+10], tokenizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 1608.18it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "[1 1 0 0 1 1 0 1 1 0]\n",
            "긍정 리뷰입니다.\n",
            "\n",
            "긍정 리뷰입니다.\n",
            "\n",
            "부정 리뷰입니다.\n",
            "\n",
            "부정 리뷰입니다.\n",
            "\n",
            "긍정 리뷰입니다.\n",
            "\n",
            "긍정 리뷰입니다.\n",
            "\n",
            "부정 리뷰입니다.\n",
            "\n",
            "긍정 리뷰입니다.\n",
            "\n",
            "긍정 리뷰입니다.\n",
            "\n",
            "부정 리뷰입니다.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}